{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcda Setup and Installation\n",
    "\n",
    "First, let's install all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "# Install LlamaIndex packages for enhanced document processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udd27 Core Imports and Configuration\n",
    "\n",
    "We import all necessary libraries and configure the LLM and embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from gradio_pdf import PDF\n",
    "import fitz  # PyMuPDF for efficient PDF handling\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# LlamaIndex imports for enhanced document processing\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
    "\n",
    "# Configure Gemini (REPLACE WITH YOUR SECURE API KEY)\n",
    "\n",
    "GEMINI_API_KEY = \"***************************************\" \n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n",
    "\n",
    "# Initialize embedding models\n",
    "# We use 'all-MiniLM-L6-v2' for its balance of performance and speed.\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "llama_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udcc4 Data Structures for Enhanced Document Management\n",
    "\n",
    "These data classes define the structured format for handling complex document metadata, which is crucial for the metadata-aware RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageInfo:\n",
    "    \"\"\"\n",
    "    Stores information about a single page extracted from the PDF.\n",
    "    This structure is used during the initial parsing phase.\n",
    "    \"\"\"\n",
    "    page_num: int\n",
    "    text: str\n",
    "    doc_type: Optional[str] = None\n",
    "    page_in_doc: int = 0\n",
    "\n",
    "@dataclass\n",
    "class LogicalDocument:\n",
    "    \"\"\"\n",
    "    Represents a logical document (e.g., a contract, a fee sheet)\n",
    "    that may span multiple pages within a single uploaded PDF file.\n",
    "    \"\"\"\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    chunks: List[Dict] = None\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"\n",
    "    Rich metadata schema for each text chunk. This metadata is stored\n",
    "    alongside the vector embedding in the FAISS index, enabling\n",
    "    filtered and highly precise retrieval.\n",
    "    \"\"\"\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    chunk_index: int\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    embedding: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83e\udde0 Document Intelligence Functions\n",
    "\n",
    "These functions implement the core intelligence layer, responsible for classifying document types and detecting logical boundaries within a single uploaded file.\n",
    "\n",
    "1. Document Classification (classify_document_type)\n",
    "\n",
    "This function uses the Gemini LLM to analyze a text sample and categorize the document, which is essential for the query routing mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document_type(text: str, max_length: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Classify the document type based on its content using the Gemini LLM.\n",
    "    This is a key component of the Document Intelligence layer.\n",
    "    \"\"\"\n",
    "    # Truncate text if too long to avoid token limits\n",
    "    text_sample = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this document and classify it into ONE of these categories:\n",
    "    - Resume: CV, professional profile, work history\n",
    "    - Contract: Legal agreement, terms and conditions, service agreement\n",
    "    - Mortgage Contract: Home loan agreement, mortgage terms, property financing\n",
    "    - Invoice: Bill, payment request, financial statement\n",
    "    - Pay Slip: Salary statement, wage slip, earnings statement\n",
    "    - Lender Fee Sheet: Loan fees, lender charges, closing costs\n",
    "    - Land Deed: Property deed, title document, ownership certificate\n",
    "    - Bank Statement: Account statement, transaction history\n",
    "    - Tax Document: W2, 1099, tax return, tax form\n",
    "    - Insurance: Insurance policy, coverage document\n",
    "    - Report: Analysis, research document, findings\n",
    "    - Letter: Correspondence, memo, communication\n",
    "    - Form: Application, questionnaire, data entry form\n",
    "    - ID Document: Driver's license, passport, identification\n",
    "    - Medical: Medical report, prescription, health record\n",
    "    - Other: Doesn't fit other categories\n",
    "\n",
    "    Document sample:\n",
    "    {text_sample}\n",
    "\n",
    "    Respond with ONLY the category name, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        doc_type = response.text.strip()\n",
    "\n",
    "        # Normalize the response to ensure it matches one of the predefined categories\n",
    "        valid_types = [\n",
    "            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
    "            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
    "            'Insurance', 'Report', 'Letter', 'Form', 'ID Document', 'Medical'\n",
    "        ]\n",
    "        if doc_type not in valid_types:\n",
    "            return 'Other'\n",
    "        return doc_type\n",
    "    except Exception as e:\n",
    "        print(f\"Error during document classification: {e}\")\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "2. Logical Document Detection (detect_logical_documents)\n",
    "\n",
    "This function processes the raw PDF, extracts text page-by-page, and uses the classifier to identify where one logical document ends and another begins, even if they are in the same physical file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_logical_documents(pdf_path: str) -> List[LogicalDocument]:\n",
    "    \"\"\"\n",
    "    Parses a multi-page PDF, extracts text, and uses the classifier to\n",
    "    segment the file into a list of distinct LogicalDocument objects.\n",
    "    This handles the case where a single PDF contains multiple, different documents.\n",
    "    \"\"\"\n",
    "    pages: List[PageInfo] = []\n",
    "    doc_hash = hashlib.sha256(open(pdf_path, 'rb').read()).hexdigest()\n",
    "    \n",
    "    # 1. Extract text and classify the first page\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            num_pages = len(reader.pages)\n",
    "            \n",
    "            for i in range(num_pages):\n",
    "                page_text = reader.pages[i].extract_text()\n",
    "                # Only classify the first page of the PDF to set the initial document type\n",
    "                if i == 0:\n",
    "                    doc_type = classify_document_type(page_text)\n",
    "                else:\n",
    "                    # For subsequent pages, we will check for a change in type\n",
    "                    doc_type = None \n",
    "                \n",
    "                pages.append(PageInfo(page_num=i + 1, text=page_text, doc_type=doc_type))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. Segment the pages into LogicalDocuments\n",
    "    logical_documents: List[LogicalDocument] = []\n",
    "    current_doc_start_page = 1\n",
    "    current_doc_type = pages[0].doc_type if pages else 'Other'\n",
    "    current_doc_text = \"\"\n",
    "    \n",
    "    for i, page in enumerate(pages):\n",
    "        # Re-classify a page if the previous page's text is significantly different\n",
    "        # This is a heuristic to detect document boundaries\n",
    "        if i > 0 and len(page.text) > 100 and (i % 5 == 0 or len(page.text) / len(pages[i-1].text) > 2 or len(pages[i-1].text) / len(page.text) > 2):\n",
    "             new_type = classify_document_type(page.text)\n",
    "             if new_type != current_doc_type and new_type != 'Other':\n",
    "                 # Finalize the previous document\n",
    "                 logical_documents.append(LogicalDocument(\n",
    "                     doc_id=f\"{doc_hash}_{current_doc_start_page}-{i}\",\n",
    "                     doc_type=current_doc_type,\n",
    "                     page_start=current_doc_start_page,\n",
    "                     page_end=i,\n",
    "                     text=current_doc_text.strip()\n",
    "                 ))\n",
    "                 # Start a new document\n",
    "                 current_doc_start_page = i + 1\n",
    "                 current_doc_type = new_type\n",
    "                 current_doc_text = page.text\n",
    "                 continue\n",
    "\n",
    "        current_doc_text += \"\n",
    "\" + page.text\n",
    "        \n",
    "    # Finalize the last document\n",
    "    if current_doc_text:\n",
    "        logical_documents.append(LogicalDocument(\n",
    "            doc_id=f\"{doc_hash}_{current_doc_start_page}-{num_pages}\",\n",
    "            doc_type=current_doc_type,\n",
    "            page_start=current_doc_start_page,\n",
    "            page_end=num_pages,\n",
    "            text=current_doc_text.strip()\n",
    "        ))\n",
    "        \n",
    "    return logical_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udee0\ufe0f Core RAG Pipeline Functions\n",
    "\n",
    "These functions handle the ingestion, indexing, and retrieval processes, forming the backbone of the RAG system.\n",
    "\n",
    "1. Ingestion and Indexing (process_pdf_to_chunks and build_faiss_index)\n",
    "\n",
    "This is the ingestion pipeline. It takes the PDF, segments it into logical documents, chunks the text, and builds a FAISS vector index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_to_chunks(pdf_path: str) -> Tuple[List[ChunkMetadata], List[Document]]:\n",
    "    \"\"\"\n",
    "    The Ingestion Layer: Converts a PDF file into a list of structured text chunks\n",
    "    (ChunkMetadata) and LlamaIndex Document objects.\n",
    "    \"\"\"\n",
    "    logical_documents = detect_logical_documents(pdf_path)\n",
    "    all_chunks: List[ChunkMetadata] = []\n",
    "    llama_documents: List[Document] = []\n",
    "    \n",
    "    # Initialize LlamaIndex text splitter for consistent chunking\n",
    "    text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "    for doc in logical_documents:\n",
    "        # 1. Split the logical document text into smaller, manageable chunks\n",
    "        chunks = text_splitter.split_text(doc.text)\n",
    "        \n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            # 2. Generate embedding for the chunk using the Sentence Transformer model\n",
    "            embedding = embed_model.encode(chunk_text)\n",
    "            \n",
    "            # 3. Create rich metadata for the chunk\n",
    "            chunk_metadata = ChunkMetadata(\n",
    "                chunk_id=f\"{doc.doc_id}_chunk_{i}\",\n",
    "                doc_id=doc.doc_id,\n",
    "                doc_type=doc.doc_type,\n",
    "                chunk_index=i,\n",
    "                page_start=doc.page_start,\n",
    "                page_end=doc.page_end,\n",
    "                text=chunk_text,\n",
    "                embedding=embedding\n",
    "            )\n",
    "            all_chunks.append(chunk_metadata)\n",
    "            \n",
    "            # 4. Create LlamaIndex Document for the chunk\n",
    "            # We use the chunk text as the document text and store all metadata\n",
    "            llama_doc = Document(\n",
    "                text=chunk_text,\n",
    "                metadata={\n",
    "                    \"doc_id\": doc.doc_id,\n",
    "                    \"doc_type\": doc.doc_type,\n",
    "                    \"chunk_id\": chunk_metadata.chunk_id,\n",
    "                    \"page_start\": doc.page_start,\n",
    "                    \"page_end\": doc.page_end,\n",
    "                }\n",
    "            )\n",
    "            llama_documents.append(llama_doc)\n",
    "            \n",
    "    return all_chunks, llama_documents\n",
    "\n",
    "def build_faiss_index(chunks: List[ChunkMetadata]) -> Tuple[faiss.IndexFlatL2, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Builds a FAISS index from the list of chunks and prepares the metadata list.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return None, []\n",
    "\n",
    "    # Get the dimension of the embeddings\n",
    "    d = chunks[0].embedding.shape[0]\n",
    "    \n",
    "    # Create a FAISS index (FlatL2 is a simple L2 distance index)\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Prepare the embeddings matrix and metadata list\n",
    "    embeddings_matrix = np.array([chunk.embedding for chunk in chunks]).astype('float32')\n",
    "    metadata_list = [\n",
    "        {\n",
    "            \"chunk_id\": chunk.chunk_id,\n",
    "            \"doc_id\": chunk.doc_id,\n",
    "            \"doc_type\": chunk.doc_type,\n",
    "            \"page_start\": chunk.page_start,\n",
    "            \"page_end\": chunk.page_end,\n",
    "            \"text\": chunk.text\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    \n",
    "    # Add the vectors to the index\n",
    "    index.add(embeddings_matrix)\n",
    "    \n",
    "    return index, metadata_list\n",
    "\n",
    "\n",
    "2. Retrieval and Querying (retrieve_chunks and rag_query)\n",
    "\n",
    "These functions handle the core RAG logic: retrieving relevant chunks and generating a final answer using the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query: str, index: faiss.IndexFlatL2, metadata_list: List[Dict], k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant chunks from the FAISS index.\n",
    "    \"\"\"\n",
    "    # 1. Embed the query\n",
    "    query_embedding = embed_model.encode(query).astype('float32').reshape(1, -1)\n",
    "    \n",
    "    # 2. Search the FAISS index\n",
    "    D, I = index.search(query_embedding, k)  # D is distances, I is indices\n",
    "    \n",
    "    # 3. Collect the relevant chunks and their metadata\n",
    "    retrieved_chunks = [metadata_list[i] for i in I[0] if i != -1]\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "def rag_query(query: str, index: faiss.IndexFlatL2, doc_types: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    The main RAG function: retrieves context and generates the final answer.\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        return \"Error: Document index is not built. Please process a PDF first.\"\n",
    "\n",
    "    # 1. Retrieve relevant chunks\n",
    "    retrieved_chunks = retrieve_chunks(query, index, global_metadata)\n",
    "    \n",
    "    if not retrieved_chunks:\n",
    "        return \"No relevant information found in the document(s).\"\n",
    "\n",
    "    # 2. Format the context for the LLM\n",
    "    context = \"\n",
    "---\n",
    "\".join([\n",
    "        f\"Document Type: {chunk['doc_type']}\n",
    "Page Range: {chunk['page_start']}-{chunk['page_end']}\n",
    "Content: {chunk['text']}\"\n",
    "        for chunk in retrieved_chunks\n",
    "    ])\n",
    "    \n",
    "    # 3. Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert document analysis assistant. Your task is to answer the user's question\n",
    "    based ONLY on the provided context. Do not use any external knowledge.\n",
    "    \n",
    "    The document(s) analyzed are of type(s): {', '.join(doc_types)}.\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    QUESTION:\n",
    "    {query}\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 4. Generate the answer\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\u2699\ufe0f Gradio Interface Logic\n",
    "\n",
    "These functions manage the state and logic for the Gradio web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to hold the index and metadata (simulating in-memory storage)\n",
    "global_index: Optional[faiss.IndexFlatL2] = None\n",
    "global_metadata: List[Dict] = []\n",
    "global_doc_types: List[str] = []\n",
    "\n",
    "def process_and_index_pdf(pdf_file) -> Tuple[str, str, gr.Button]:\n",
    "    \"\"\"\n",
    "    Handles the file upload, processing, and index building.\n",
    "    Returns the status message and the updated state of the query button.\n",
    "    \"\"\"\n",
    "    global global_index, global_metadata, global_doc_types\n",
    "    \n",
    "    if pdf_file is None:\n",
    "        return \"Error: Please upload a PDF file.\", \"Error: Please upload a PDF file.\", gr.Button(interactive=False)\n",
    "\n",
    "    pdf_path = pdf_file.name\n",
    "    \n",
    "    try:\n",
    "        # 1. Process PDF into chunks\n",
    "        status_msg = f\"Processing PDF: {pdf_path}...\"\n",
    "        all_chunks, _ = process_pdf_to_chunks(pdf_path)\n",
    "        \n",
    "        if not all_chunks:\n",
    "            return \"Error: Could not extract content from PDF.\", \"Error: Could not extract content from PDF.\", gr.Button(interactive=False)\n",
    "\n",
    "        # 2. Build FAISS index\n",
    "        status_msg += f\"\n",
    "Building FAISS index with {len(all_chunks)} chunks...\"\n",
    "        new_index, new_metadata = build_faiss_index(all_chunks)\n",
    "        \n",
    "        # 3. Update global state\n",
    "        global_index = new_index\n",
    "        global_metadata = new_metadata\n",
    "        global_doc_types = list(set(chunk.doc_type for chunk in all_chunks))\n",
    "        \n",
    "        \n",
    "        return final_status, final_status, gr.Button(interactive=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"An unexpected error occurred during indexing: {e}\"\n",
    "        return error_msg, error_msg, gr.Button(interactive=False)\n",
    "\n",
    "def answer_question(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Handles the user query and returns the RAG answer.\n",
    "    \"\"\"\n",
    "    if global_index is None:\n",
    "        return \"Error: Index not built. Please process a PDF first.\"\n",
    "    if not query:\n",
    "        return \"Please enter a question.\"\n",
    "        \n",
    "    # Execute the full RAG query\n",
    "    answer = rag_query(query, global_index, global_doc_types)\n",
    "    return answer\n",
    "\n",
    "# Define the Gradio Interface\n",
    "with gr.Blocks(title=\"Intelligent RAG Document Q&A System\") as demo:\n",
    "    gr.Markdown(\"# Intelligent RAG Document Q&A System\")\n",
    "    gr.Markdown(\"Upload a PDF (potentially containing multiple document types) and ask questions about its content.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # PDF Viewer and Uploader\n",
    "        pdf_viewer = PDF(label=\"Uploaded Document Preview\", height=600)\n",
    "        \n",
    "        with gr.Column():\n",
    "            # Uploader and Indexing Button\n",
    "            pdf_upload = gr.File(label=\"Upload PDF Document\", file_types=[\".pdf\"])\n",
    "            index_button = gr.Button(\"Process and Build Index\")\n",
    "            indexing_status = gr.Textbox(label=\"Indexing Status\", lines=5)\n",
    "            \n",
    "            # Q&A Section\n",
    "            gr.Markdown(\"## Ask a Question\")\n",
    "            question_input = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., What is the interest rate on the mortgage contract?\")\n",
    "            answer_output = gr.Textbox(label=\"RAG Answer\", lines=10)\n",
    "            \n",
    "            # The Q&A button is initially disabled until the index is built\n",
    "            query_button = gr.Button(\"Get Answer\", interactive=False)\n",
    "\n",
    "    # Event Handlers\n",
    "    index_button.click(\n",
    "        fn=process_and_index_pdf,\n",
    "        inputs=[pdf_upload],\n",
    "        outputs=[indexing_status, indexing_status, query_button] # Update status and enable query button\n",
    "    )\n",
    "    \n",
    "    query_button.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input],\n",
    "        outputs=[answer_output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\ude80 Launch the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interface is launched with share=True for easy access in Colab\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}